Q1. Which word has the highest frequency of occurrence in the document?
------------------------------------------------------------------------------------
import java.io.IOException;
import java.util.HashSet;
import java.util.Set;
import java.util.StringTokenizer;
import java.util.regex.Matcher;
import java.util.regex.Pattern;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;

public class Q1_1 extends Configured implements Tool{
	

	
	public static class ShakespeareMapper
     extends Mapper<LongWritable, Text, Text, IntWritable>{

private final static IntWritable one = new IntWritable(1);
private Text word = new Text();
static Set<String> s = new HashSet<>();

 public void map(LongWritable key, Text value, Context context
                 ) throws IOException, InterruptedException {
   StringTokenizer itr = new StringTokenizer(value.toString());
   int len;
   Pattern pattern;
   Matcher matcher;
   while (itr.hasMoreTokens()) {
 	  String myword = itr.nextToken().toLowerCase();
 	  len = myword.length();
 	  pattern = Pattern.compile("[a-z']{1,"+len+"}");
 	  matcher = pattern.matcher(myword);
 	  if(matcher.find())
 	  {
 		  myword = matcher.group();
 		  word.set(myword);
 	      context.write(word, one);
 	  }
 	}
 }
}


	
	
public static class IntSumReducer
    extends Reducer<Text,IntWritable,Text,IntWritable> {

 static int max;
 static long count;
 static String k;
 
 public void reduce(Text key, Iterable<IntWritable> values,
                    Context context
                    ) throws IOException, InterruptedException {
   int sum = 0;
   for (IntWritable val : values) {
     sum += val.get();
   }
   
   /*Configuration conf = context.getConfiguration();
   Cluster cluster = new Cluster(conf);
   Job currentJob = cluster.getJob(context.getJobID());
   long reduceSize = currentJob.getCounters().findCounter(TaskCounter.REDUCE_INPUT_GROUPS).getValue();*/
   
   //long reduceSize = context.getConfiguration().getLong("numUniqueWords", -1);
   
   if(sum>max)
   {
	   max = sum;
	   k = key.toString();
   }
   
 }
 
 public void cleanup(Context context) throws IOException, InterruptedException
 {
	 context.write(new Text(k), new IntWritable(max));
 }
}

public int run(String[] args) throws Exception
{
	 Configuration conf = new Configuration();
	 //conf.setLong("numUniqueWords", 0);
	 Job job = Job.getInstance(conf, "word count shakespeare");
	 job.setJarByClass(Q1_1.class);
	 job.setMapperClass(ShakespeareMapper.class);
	 job.setReducerClass(IntSumReducer.class);
	 job.setNumReduceTasks(1);
	 job.setMapOutputKeyClass(Text.class);
	 job.setMapOutputValueClass(IntWritable.class);
	 job.setOutputKeyClass(Text.class);
	 job.setOutputValueClass(IntWritable.class);
	 FileInputFormat.addInputPath(job, new Path(args[0]));
	 FileOutputFormat.setOutputPath(job, new Path(args[1]));
	 System.exit(job.waitForCompletion(true) ? 0 : 1);
	 return 0;
}



public static void main(String[] args) throws Exception {
	ToolRunner.run(new Configuration(), new Q1_1(),args);
    System.exit(0);
}
}
o/p:

the	30186

Q2. What is the frequency of occurrence of the word ‘Romeo’? (Ignore cases and don't
remove punctuation marks from any words.)
------------------------------------------------------------------------------------
import java.io.IOException;
import java.util.StringTokenizer;
import java.util.regex.Matcher;
import java.util.regex.Pattern;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;


public class Q1_2 {
	public static class TokenizerMapper
    extends Mapper<LongWritable, Text, Text, IntWritable>{

private final static IntWritable one = new IntWritable(1);
private Text word = new Text();

public void map(LongWritable key, Text value, Context context
                 ) throws IOException, InterruptedException {
   StringTokenizer itr = new StringTokenizer(value.toString());
   Pattern pattern;
   Matcher matcher;
   while (itr.hasMoreTokens()) {
 	  String myword = itr.nextToken().toLowerCase();
 	  pattern = Pattern.compile("romeo");
 	  matcher = pattern.matcher(myword);
 	  if(matcher.find())
 	  {
 		  myword = matcher.group();
 		  word.set(myword);
 	      context.write(word, one);
 	  }
 	}
 }
}

public static class IntSumReducer
    extends Reducer<Text,IntWritable,Text,IntWritable> {
 private IntWritable result = new IntWritable();
 public void reduce(Text key, Iterable<IntWritable> values,
                    Context context
                    ) throws IOException, InterruptedException {
   int sum = 0;
   for (IntWritable val : values) {
     sum += val.get();
   }
    result.set(sum);
   	context.write(key, result);
   
 }
}

public static void main(String[] args) throws Exception {
 Configuration conf = new Configuration();
 Job job = Job.getInstance(conf, "word count");
 job.setJarByClass(Q1_2.class);
 job.setMapperClass(TokenizerMapper.class);
 job.setReducerClass(IntSumReducer.class);
 job.setNumReduceTasks(1);
 
 job.setMapOutputKeyClass(Text.class);
 job.setMapOutputValueClass(IntWritable.class);
 job.setOutputKeyClass(Text.class);
 job.setOutputValueClass(IntWritable.class);
 FileInputFormat.addInputPath(job, new Path(args[0]));
 FileOutputFormat.setOutputPath(job, new Path(args[1]));
 System.exit(job.waitForCompletion(true) ? 0 : 1);
}
}

o/p:
romeo	317

Q3.What is the frequency of the phrase "circumference." in the data set? (You do not
need to remove the punctuation marks from the words.)
---------------------------------------------------------------------------------------------
import java.io.IOException;
import java.util.StringTokenizer;
import java.util.regex.Matcher;
import java.util.regex.Pattern;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;


public class Q1_3 {
	public static class TokenizerMapper
    extends Mapper<LongWritable, Text, Text, IntWritable>{

private final static IntWritable one = new IntWritable(1);
private Text word = new Text();

public void map(LongWritable key, Text value, Context context
                 ) throws IOException, InterruptedException {
   StringTokenizer itr = new StringTokenizer(value.toString());
   Pattern pattern;
   Matcher matcher;
   while (itr.hasMoreTokens()) {
 	  String myword = itr.nextToken().toLowerCase();
 	  pattern = Pattern.compile("circumference");
 	  matcher = pattern.matcher(myword);
 	  if(matcher.find())
 	  {
 		  myword = matcher.group();
 		  word.set(myword);
 	      context.write(word, one);
 	  }
 	}
 }
}

public static class IntSumReducer
    extends Reducer<Text,IntWritable,Text,IntWritable> {
 private IntWritable result = new IntWritable();
 public void reduce(Text key, Iterable<IntWritable> values,
                    Context context
                    ) throws IOException, InterruptedException {
   int sum = 0;
   for (IntWritable val : values) {
     sum += val.get();
   }
    result.set(sum);
   	context.write(key, result);
   
 }
}

public static void main(String[] args) throws Exception {
 Configuration conf = new Configuration();
 Job job = Job.getInstance(conf, "word count");
 job.setJarByClass(Q1_3.class);
 job.setMapperClass(TokenizerMapper.class);
 job.setReducerClass(IntSumReducer.class);
 job.setNumReduceTasks(1);
 
 job.setMapOutputKeyClass(Text.class);
 job.setMapOutputValueClass(IntWritable.class);
 job.setOutputKeyClass(Text.class);
 job.setOutputValueClass(IntWritable.class);
 FileInputFormat.addInputPath(job, new Path(args[0]));
 FileOutputFormat.setOutputPath(job, new Path(args[1]));
 System.exit(job.waitForCompletion(true) ? 0 : 1);
}
}

o/p:
circumference	3
===========================================================================================================

Q1. Count the number of unique ordered pairs of origin and destination (Origin,
Destination) present in the dataset, i.e., for two flights, either the origin or the destination
differs.
---------------------------------------------------------------------------------------------------
import java.io.IOException;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;

public class Q2_1 extends Configured implements Tool{
	

	
	public static class RoutesMapper
     extends Mapper<LongWritable, Text, Text, IntWritable>{

private final static IntWritable one = new IntWritable(1);
String origin;
String destination;
String[] values;

 public void map(LongWritable key, Text value, Context context
                 ) throws IOException, InterruptedException {
   
   values = value.toString().split(",");
   origin = values[17];
   destination = values[18];
   context.write(new Text(origin+destination), one);
   
 }
}


public static class RoutesReducer
    extends Reducer<Text,IntWritable,Text,LongWritable> {

 static long count;
 
 public void reduce(Text key, Iterable<IntWritable> values,Context context) throws IOException, InterruptedException {
   
   count++;
   
 }
 
 
 public void cleanup(Context context) throws IOException, InterruptedException
 {
	 context.write(new Text("Number of unique flight routes"), new LongWritable(count));
 }
}

public int run(String[] args) throws Exception
{
	 Configuration conf = new Configuration();
	 conf.set("mapreduce.output.textoutputformat.separator",":");
	 Job job = Job.getInstance(conf, "unique routes count");
	 job.setJarByClass(Q2_1.class);
	 job.setMapperClass(RoutesMapper.class);
	 job.setReducerClass(RoutesReducer.class);
	 job.setNumReduceTasks(1);
	 job.setMapOutputKeyClass(Text.class);
	 job.setMapOutputValueClass(IntWritable.class);
	 job.setOutputFormatClass(TextOutputFormat.class);
	 job.setOutputKeyClass(Text.class);
	 job.setOutputValueClass(LongWritable.class);
	 FileInputFormat.addInputPath(job, new Path(args[0]));
	 FileOutputFormat.setOutputPath(job, new Path(args[1]));
	 System.exit(job.waitForCompletion(true) ? 0 : 1);
	 return 0;
}



public static void main(String[] args) throws Exception {
	ToolRunner.run(new Configuration(), new Q2_1(),args);
    System.exit(0);
}
}

O/p:Number of unique flight routes:1404

Q2. What is the airport code and the number of flights corresponding to that airport, with the maximum number of outgoing flights in the year 2004?
--------------------------------------------------------------------------------------------------------------------------------------------
import java.io.IOException;
import java.util.ArrayList;
import java.util.List;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;

public class Q2_2 extends Configured implements Tool{
	

	
	public static class MaxFlightMapper
     extends Mapper<LongWritable, Text, Text, Text>{

String origin;
String destination;
String year;
String[] values;

 public void map(LongWritable key, Text value, Context context
                 ) throws IOException, InterruptedException {
   
   values = value.toString().split(",");
   origin = values[17];
   destination = values[18];
   year = values[1];
   context.write(new Text(origin),new Text(year+":"+origin+":"+destination));
   
 }
}


public static class MaxFlightReducer
    extends Reducer<Text,Text,Text,IntWritable> {

 static long count;
 static int max;
 static String airportCode;
 static List<String> routes=new ArrayList<>();
 
 public void reduce(Text key, Iterable<Text> values,Context context) throws IOException, InterruptedException {
   
   int sum=0;
   String s;
   for(Text t:values)
   {
	  s=t.toString();
	  routes.add(s);
	  if(s.substring(0,s.indexOf(":")).equals("2004"))
	  {
		  sum++;
	  }
   }
   
   if(sum>max)
   {
	   max=sum;
	   airportCode=key.toString();
   }
   
 }
 
 
 public void cleanup(Context context) throws IOException, InterruptedException
 {
	 int totalFlights=0;
	 for(String s:routes)
	 {
		 if(s.contains(airportCode))
			 totalFlights++;
	 }
	 context.write(new Text(airportCode), new IntWritable(totalFlights));
 }
 
}

public int run(String[] args) throws Exception
{
	 Configuration conf = new Configuration();
	 conf.set("mapreduce.output.textoutputformat.separator",":");
	 Job job = Job.getInstance(conf, "unique routes count");
	 job.setJarByClass(Q2_2.class);
	 job.setMapperClass(MaxFlightMapper.class);
	 job.setReducerClass(MaxFlightReducer.class);
	 job.setNumReduceTasks(1);
	 job.setMapOutputKeyClass(Text.class);
	 job.setMapOutputValueClass(Text.class);
	 job.setOutputFormatClass(TextOutputFormat.class);
	 job.setOutputKeyClass(Text.class);
	 job.setOutputValueClass(IntWritable.class);
	 FileInputFormat.addInputPath(job, new Path(args[0]));
	 FileOutputFormat.setOutputPath(job, new Path(args[1]));
	 System.exit(job.waitForCompletion(true) ? 0 : 1);
	 return 0;
}



public static void main(String[] args) throws Exception {
	ToolRunner.run(new Configuration(), new Q2_2(),args);
    System.exit(0);
}
}

O/p:ORD:21057

===============================================================================================
Q1. Which player scored the highest number of centuries?
------------------------------------------------------------------
import java.io.IOException;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;

public class Q3_1 extends Configured implements Tool{
	

	
public static class MaxCenturiesMapper
     extends Mapper<LongWritable, Text, Text, IntWritable>{

private final static IntWritable one = new IntWritable(1);
String player;
int runs;
String[] values;

 public void map(LongWritable key, Text value, Context context
                 ) throws IOException, InterruptedException {
   
   values = value.toString().split(",");
   player = values[1];
   runs = Integer.parseInt(values[2]);
   
   if(runs>=100)
       context.write(new Text(player),one);
   }
}


public static class MaxCenturiesReducer
    extends Reducer<Text,IntWritable,Text,IntWritable> {

 
 static int max;
 static String playerName;
 
 
 public void reduce(Text key, Iterable<IntWritable> values,Context context) throws IOException, InterruptedException {
   
   int count=0;
   for(IntWritable i:values)
   {
	  count += i.get();
   }
   
   if(count>max)
   {
	   max=count;
	   playerName=key.toString();
   }
   
 }
 
 
 public void cleanup(Context context) throws IOException, InterruptedException
 {
	 context.write(new Text(playerName), new IntWritable(max));
 }
 
}

public int run(String[] args) throws Exception
{
	 Configuration conf = new Configuration();
	 conf.set("mapreduce.output.textoutputformat.separator",":");
	 Job job = Job.getInstance(conf, "maximum centuries by player");
	 job.setJarByClass(Q3_1.class);
	 job.setMapperClass(MaxCenturiesMapper.class);
	 job.setReducerClass(MaxCenturiesReducer.class);
	 job.setNumReduceTasks(1);
	 job.setMapOutputKeyClass(Text.class);
	 job.setMapOutputValueClass(IntWritable.class);
	 job.setOutputFormatClass(TextOutputFormat.class);
	 job.setOutputKeyClass(Text.class);
	 job.setOutputValueClass(IntWritable.class);
	 FileInputFormat.addInputPath(job, new Path(args[0]));
	 FileOutputFormat.setOutputPath(job, new Path(args[1]));
	 System.exit(job.waitForCompletion(true) ? 0 : 1);
	 return 0;
}



public static void main(String[] args) throws Exception {
	ToolRunner.run(new Configuration(), new Q3_1(),args);
    System.exit(0);
}
}

O/p:Sachin R Tendulkar:48

Q2. In which year did Indian players score the maximum number of centuries?
-------------------------------------------------------------------------------------------------
import java.io.IOException;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;

public class Q3_2 extends Configured implements Tool{
	

	
public static class MaxCenturiesIndiaMapper
     extends Mapper<LongWritable, Text, Text, IntWritable>{

private final static IntWritable one = new IntWritable(1);
String year;
int runs;
String[] values;

 public void map(LongWritable key, Text value, Context context
                 ) throws IOException, InterruptedException {
   
   values = value.toString().split(",");
   year = values[3];
   year = year.substring(year.lastIndexOf("-")+1,year.length());
   runs = Integer.parseInt(values[2]);
   
   if(values[0].trim().equalsIgnoreCase("India") && runs>=100)
       context.write(new Text(year),one);
   }
}


public static class MaxCenturiesIndiaReducer
    extends Reducer<Text,IntWritable,Text,IntWritable> {

 
 static int max;
 static String year;
 
 
 public void reduce(Text key, Iterable<IntWritable> values,Context context) throws IOException, InterruptedException {
   
   int count=0;
   for(IntWritable i:values)
   {
	  count += i.get();
   }
   
   if(count>max)
   {
	   max=count;
	   year=key.toString();
   }
   
 }
 
 
 public void cleanup(Context context) throws IOException, InterruptedException
 {
	 context.write(new Text(year), new IntWritable(max));
 }
 
}

public int run(String[] args) throws Exception
{
	 Configuration conf = new Configuration();
	 conf.set("mapreduce.output.textoutputformat.separator",":");
	 Job job = Job.getInstance(conf, "maximum centuries by player");
	 job.setJarByClass(Q3_2.class);
	 job.setMapperClass(MaxCenturiesIndiaMapper.class);
	 job.setReducerClass(MaxCenturiesIndiaReducer.class);
	 job.setNumReduceTasks(1);
	 job.setMapOutputKeyClass(Text.class);
	 job.setMapOutputValueClass(IntWritable.class);
	 job.setOutputFormatClass(TextOutputFormat.class);
	 job.setOutputKeyClass(Text.class);
	 job.setOutputValueClass(IntWritable.class);
	 FileInputFormat.addInputPath(job, new Path(args[0]));
	 FileOutputFormat.setOutputPath(job, new Path(args[1]));
	 System.exit(job.waitForCompletion(true) ? 0 : 1);
	 return 0;
}



public static void main(String[] args) throws Exception {
	ToolRunner.run(new Configuration(), new Q3_2(),args);
    System.exit(0);
}
}

O/p:1998:18

